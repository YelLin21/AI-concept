{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mCuo0fnA68ic",
        "outputId": "44871d19-6d1b-4e63-a6b1-eec4cf7f9dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150 - Loss: 5.1244\n",
            "Epoch 2/150 - Loss: 4.7738\n",
            "Epoch 3/150 - Loss: 4.6458\n",
            "Epoch 4/150 - Loss: 4.5418\n",
            "Epoch 5/150 - Loss: 4.3921\n",
            "Epoch 6/150 - Loss: 4.1644\n",
            "Epoch 7/150 - Loss: 3.9271\n",
            "Epoch 8/150 - Loss: 3.7265\n",
            "Epoch 9/150 - Loss: 3.4906\n",
            "Epoch 10/150 - Loss: 3.2718\n",
            "Epoch 11/150 - Loss: 3.0767\n",
            "Epoch 12/150 - Loss: 2.8436\n",
            "Epoch 13/150 - Loss: 2.6679\n",
            "Epoch 14/150 - Loss: 2.4869\n",
            "Epoch 15/150 - Loss: 2.3087\n",
            "Epoch 16/150 - Loss: 2.1736\n",
            "Epoch 17/150 - Loss: 1.9987\n",
            "Epoch 18/150 - Loss: 1.8486\n",
            "Epoch 19/150 - Loss: 1.7385\n",
            "Epoch 20/150 - Loss: 1.6138\n",
            "Epoch 21/150 - Loss: 1.5365\n",
            "Epoch 22/150 - Loss: 1.3929\n",
            "Epoch 23/150 - Loss: 1.3415\n",
            "Epoch 24/150 - Loss: 1.2165\n",
            "Epoch 25/150 - Loss: 1.1510\n",
            "Epoch 26/150 - Loss: 1.0565\n",
            "Epoch 27/150 - Loss: 1.0023\n",
            "Epoch 28/150 - Loss: 0.9275\n",
            "Epoch 29/150 - Loss: 0.9085\n",
            "Epoch 30/150 - Loss: 0.8331\n",
            "Epoch 31/150 - Loss: 0.7732\n",
            "Epoch 32/150 - Loss: 0.7517\n",
            "Epoch 33/150 - Loss: 0.7038\n",
            "Epoch 34/150 - Loss: 0.6591\n",
            "Epoch 35/150 - Loss: 0.6251\n",
            "Epoch 36/150 - Loss: 0.5929\n",
            "Epoch 37/150 - Loss: 0.5562\n",
            "Epoch 38/150 - Loss: 0.5485\n",
            "Epoch 39/150 - Loss: 0.5166\n",
            "Epoch 40/150 - Loss: 0.4711\n",
            "Epoch 41/150 - Loss: 0.4622\n",
            "Epoch 42/150 - Loss: 0.4420\n",
            "Epoch 43/150 - Loss: 0.4089\n",
            "Epoch 44/150 - Loss: 0.3863\n",
            "Epoch 45/150 - Loss: 0.3589\n",
            "Epoch 46/150 - Loss: 0.3403\n",
            "Epoch 47/150 - Loss: 0.3308\n",
            "Epoch 48/150 - Loss: 0.3313\n",
            "Epoch 49/150 - Loss: 0.2945\n",
            "Epoch 50/150 - Loss: 0.2730\n",
            "Epoch 51/150 - Loss: 0.2533\n",
            "Epoch 52/150 - Loss: 0.2462\n",
            "Epoch 53/150 - Loss: 0.2351\n",
            "Epoch 54/150 - Loss: 0.2276\n",
            "Epoch 55/150 - Loss: 0.2050\n",
            "Epoch 56/150 - Loss: 0.1836\n",
            "Epoch 57/150 - Loss: 0.1844\n",
            "Epoch 58/150 - Loss: 0.1683\n",
            "Epoch 59/150 - Loss: 0.1670\n",
            "Epoch 60/150 - Loss: 0.1646\n",
            "Epoch 61/150 - Loss: 0.1421\n",
            "Epoch 62/150 - Loss: 0.1433\n",
            "Epoch 63/150 - Loss: 0.1323\n",
            "Epoch 64/150 - Loss: 0.1229\n",
            "Epoch 65/150 - Loss: 0.1258\n",
            "Epoch 66/150 - Loss: 0.1170\n",
            "Epoch 67/150 - Loss: 0.1102\n",
            "Epoch 68/150 - Loss: 0.1047\n",
            "Epoch 69/150 - Loss: 0.0997\n",
            "Epoch 70/150 - Loss: 0.0960\n",
            "Epoch 71/150 - Loss: 0.0903\n",
            "Epoch 72/150 - Loss: 0.0908\n",
            "Epoch 73/150 - Loss: 0.0821\n",
            "Epoch 74/150 - Loss: 0.0811\n",
            "Epoch 75/150 - Loss: 0.0747\n",
            "Epoch 76/150 - Loss: 0.0744\n",
            "Epoch 77/150 - Loss: 0.0713\n",
            "Epoch 78/150 - Loss: 0.0671\n",
            "Epoch 79/150 - Loss: 0.0693\n",
            "Epoch 80/150 - Loss: 0.0603\n",
            "Epoch 81/150 - Loss: 0.0664\n",
            "Epoch 82/150 - Loss: 0.0633\n",
            "Epoch 83/150 - Loss: 0.0578\n",
            "Epoch 84/150 - Loss: 0.0566\n",
            "Epoch 85/150 - Loss: 0.0546\n",
            "Epoch 86/150 - Loss: 0.0565\n",
            "Epoch 87/150 - Loss: 0.0525\n",
            "Epoch 88/150 - Loss: 0.0517\n",
            "Epoch 89/150 - Loss: 0.0533\n",
            "Epoch 90/150 - Loss: 0.0497\n",
            "Epoch 91/150 - Loss: 0.0474\n",
            "Epoch 92/150 - Loss: 0.0458\n",
            "Epoch 93/150 - Loss: 0.0449\n",
            "Epoch 94/150 - Loss: 0.0436\n",
            "Epoch 95/150 - Loss: 0.0441\n",
            "Epoch 96/150 - Loss: 0.0426\n",
            "Epoch 97/150 - Loss: 0.0410\n",
            "Epoch 98/150 - Loss: 0.0413\n",
            "Epoch 99/150 - Loss: 0.0394\n",
            "Epoch 100/150 - Loss: 0.0373\n",
            "Epoch 101/150 - Loss: 0.0395\n",
            "Epoch 102/150 - Loss: 0.0362\n",
            "Epoch 103/150 - Loss: 0.0365\n",
            "Epoch 104/150 - Loss: 0.0349\n",
            "Epoch 105/150 - Loss: 0.0350\n",
            "Epoch 106/150 - Loss: 0.0340\n",
            "Epoch 107/150 - Loss: 0.0316\n",
            "Epoch 108/150 - Loss: 0.0340\n",
            "Epoch 109/150 - Loss: 0.0323\n",
            "Epoch 110/150 - Loss: 0.0314\n",
            "Epoch 111/150 - Loss: 0.0308\n",
            "Epoch 112/150 - Loss: 0.0304\n",
            "Epoch 113/150 - Loss: 0.0298\n",
            "Epoch 114/150 - Loss: 0.0284\n",
            "Epoch 115/150 - Loss: 0.0356\n",
            "Epoch 116/150 - Loss: 0.0438\n",
            "Epoch 117/150 - Loss: 0.0374\n",
            "Epoch 118/150 - Loss: 0.0330\n",
            "Epoch 119/150 - Loss: 0.0451\n",
            "Epoch 120/150 - Loss: 0.0321\n",
            "Epoch 121/150 - Loss: 0.0296\n",
            "Epoch 122/150 - Loss: 0.0290\n",
            "Epoch 123/150 - Loss: 0.0274\n",
            "Epoch 124/150 - Loss: 0.0271\n",
            "Epoch 125/150 - Loss: 0.0255\n",
            "Epoch 126/150 - Loss: 0.0244\n",
            "Epoch 127/150 - Loss: 0.0227\n",
            "Epoch 128/150 - Loss: 0.0234\n",
            "Epoch 129/150 - Loss: 0.0218\n",
            "Epoch 130/150 - Loss: 0.0226\n",
            "Epoch 131/150 - Loss: 0.0221\n",
            "Epoch 132/150 - Loss: 0.0209\n",
            "Epoch 133/150 - Loss: 0.0220\n",
            "Epoch 134/150 - Loss: 0.0209\n",
            "Epoch 135/150 - Loss: 0.0206\n",
            "Epoch 136/150 - Loss: 0.0205\n",
            "Epoch 137/150 - Loss: 0.0223\n",
            "Epoch 138/150 - Loss: 0.0200\n",
            "Epoch 139/150 - Loss: 0.0217\n",
            "Epoch 140/150 - Loss: 0.0189\n",
            "Epoch 141/150 - Loss: 0.0192\n",
            "Epoch 142/150 - Loss: 0.0195\n",
            "Epoch 143/150 - Loss: 0.0176\n",
            "Epoch 144/150 - Loss: 0.0178\n",
            "Epoch 145/150 - Loss: 0.0171\n",
            "Epoch 146/150 - Loss: 0.0170\n",
            "Epoch 147/150 - Loss: 0.0180\n",
            "Epoch 148/150 - Loss: 0.0170\n",
            "Epoch 149/150 - Loss: 0.0169\n",
            "Epoch 150/150 - Loss: 0.0162\n",
            "You: hi\n",
            "Bot: hi there ! let ' s discuss educational technology .\n",
            "You: what is educational technology\n",
            "Bot: educational technology ( edtech ) involves using digital tools to enhance learning .\n",
            "You: how do adaptive learning platforms work?\n",
            "Bot: they personalize lessons based on a student ’ s progress using ai algorithms .\n",
            "You: what is a flipped classroom?\n",
            "Bot: a flipped classroom lets students learn theory at home and do practical work in class .\n",
            "You: how does virtual reality benefit education?\n",
            "Bot: vr immerses students in realistic simulations , improving engagement and retention .\n",
            "You: how does mobile learning help?\n",
            "Bot: mobile learning enables education anywhere using smartphones and tablets .\n",
            "You: how does ai assist in education?\n",
            "Bot: ai provides personalized tutoring , grading automation , and adaptive learning .\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9ef8cf86db6e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;31m# Test chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define special tokens\n",
        "SOS_TOKEN = \"<sos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "\n",
        "conversation_pairs = [\n",
        "    (\"hello\", \"Hi there! Let's discuss Educational Technology.\"),\n",
        "    (\"what is educational technology?\", \"Educational Technology (EdTech) involves using digital tools to enhance learning.\"),\n",
        "    (\"how do adaptive learning platforms work?\", \"They personalize lessons based on a student’s progress using AI algorithms.\"),\n",
        "    (\"what is a flipped classroom?\", \"A flipped classroom lets students learn theory at home and do practical work in class.\"),\n",
        "    (\"which lms platforms are popular?\", \"Popular LMS platforms include Moodle, Canvas, and Blackboard.\"),\n",
        "    (\"how does virtual reality benefit education?\", \"VR immerses students in realistic simulations, improving engagement and retention.\"),\n",
        "    (\"why is gamification used in edtech?\", \"Gamification boosts motivation through points, badges, and leaderboards.\"),\n",
        "    (\"what are open educational resources?\", \"OER are free, openly licensed learning materials accessible to anyone.\"),\n",
        "    (\"how does mobile learning help?\", \"Mobile learning enables education anywhere using smartphones and tablets.\"),\n",
        "    (\"how does ai assist in education?\", \"AI provides personalized tutoring, grading automation, and adaptive learning.\"),\n",
        "    (\"why do teachers need training for edtech?\", \"Teacher training is crucial for effective tech integration in classrooms.\"),\n",
        "    (\"how is edtech success measured?\", \"Success is measured by student engagement, performance, and feedback.\"),\n",
        "    (\"what are the advantages of discussion forums?\", \"They allow online discussions, extending learning beyond the classroom.\"),\n",
        "    (\"how does edtech support students with disabilities?\", \"Assistive tech like screen readers and speech-to-text aids learning.\"),\n",
        "    (\"what is the role of cloud computing in edtech?\", \"Cloud storage provides remote access to educational content anytime.\"),\n",
        "    (\"what are common edtech challenges in rural areas?\", \"Limited internet, high costs, and lack of digital literacy are barriers.\"),\n",
        "    (\"what are micro-credentials?\", \"Micro-credentials certify mastery of specific skills, often through online courses.\"),\n",
        "    (\"why use online quizzes?\", \"Online quizzes offer instant feedback and help track student progress.\"),\n",
        "    (\"what are the latest edtech trends?\", \"AI-driven personalization, VR learning, and blockchain for credentials.\"),\n",
        "    (\"goodbye\", \"Goodbye! Keep exploring Educational Technology.\"),\n",
        "]\n",
        "\n",
        "# Build vocabulary function\n",
        "def build_vocab(conversation_pairs, min_freq=1):\n",
        "    word_freq = {}\n",
        "    for (inp, out) in conversation_pairs:\n",
        "        tokens_in = re.findall(r\"\\w+|\\S\", inp.lower())\n",
        "        tokens_out = re.findall(r\"\\w+|\\S\", out.lower())\n",
        "\n",
        "        for t in tokens_in + tokens_out:\n",
        "            word_freq[t] = word_freq.get(t, 0) + 1\n",
        "\n",
        "    vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
        "    for w, freq in word_freq.items():\n",
        "        if freq >= min_freq and w not in vocab:\n",
        "            vocab.append(w)\n",
        "\n",
        "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "    idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "    return vocab, word2idx, idx2word\n",
        "\n",
        "# Initialize vocabulary\n",
        "vocab, word2idx, idx2word = build_vocab(conversation_pairs)\n",
        "\n",
        "# Sentence encoding function\n",
        "def encode_sentence(sentence, word2idx, max_len=60):\n",
        "    tokens = re.findall(r\"\\w+|\\S\", sentence.lower())\n",
        "    encoded = [word2idx[SOS_TOKEN]]\n",
        "    for t in tokens:\n",
        "        encoded.append(word2idx.get(t, word2idx[UNK_TOKEN]))\n",
        "    encoded.append(word2idx[EOS_TOKEN])\n",
        "\n",
        "    return encoded + [word2idx[PAD_TOKEN]] * (max_len - len(encoded)) if len(encoded) < max_len else encoded[:max_len]\n",
        "\n",
        "# Create dataset\n",
        "def create_dataset(conversation_pairs, word2idx, max_len=60):\n",
        "    data = [(encode_sentence(inp, word2idx, max_len), encode_sentence(out, word2idx, max_len)) for inp, out in conversation_pairs]\n",
        "    return data\n",
        "\n",
        "dataset = create_dataset(conversation_pairs, word2idx)\n",
        "\n",
        "# Transformer-based chatbot model\n",
        "class TransformerChat(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, n_heads=4, num_layers=3, max_len=60):\n",
        "        super().__init__()\n",
        "        self.emb_inp = nn.Embedding(vocab_size, d_model)\n",
        "        self.emb_out = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=256,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb, tgt_emb = self.emb_inp(src).transpose(0, 1), self.emb_out(tgt).transpose(0, 1)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_emb.size(0)).to(src_emb.device)\n",
        "        return self.fc_out(self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)).transpose(0, 1)\n",
        "\n",
        "# Dataset preparation\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0], dtype=torch.long), torch.tensor(self.data[idx][1], dtype=torch.long)\n",
        "\n",
        "# Training setup\n",
        "batch_size = 4\n",
        "chat_loader = DataLoader(ChatDataset(dataset), batch_size=batch_size, shuffle=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerChat(len(vocab)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[PAD_TOKEN])\n",
        "epochs = 150  # Increased epochs\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in chat_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        logits = model(src, tgt[:, :-1])\n",
        "        loss = criterion(logits.reshape(-1, len(vocab)), tgt[:, 1:].reshape(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(chat_loader):.4f}\")\n",
        "\n",
        "# Generate response function\n",
        "def generate_reply(model, input_str, word2idx, idx2word, max_len=60):\n",
        "    model.eval()\n",
        "    src_tensor = torch.tensor(encode_sentence(input_str, word2idx, max_len), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    tgt_tokens = [word2idx[SOS_TOKEN]]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_tensor = torch.tensor(tgt_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "        logits = model(src_tensor, tgt_tensor)\n",
        "        next_token_id = logits[0, -1, :].argmax(dim=-1).item()\n",
        "        tgt_tokens.append(next_token_id)\n",
        "        if idx2word[next_token_id] == EOS_TOKEN:\n",
        "            break\n",
        "\n",
        "    return \" \".join(idx2word[t] for t in tgt_tokens[1:] if idx2word[t] not in [PAD_TOKEN, EOS_TOKEN])\n",
        "\n",
        "# Test chatbot\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "    print(\"Bot:\", generate_reply(model, user_input, word2idx, idx2word))\n"
      ]
    }
  ]
}